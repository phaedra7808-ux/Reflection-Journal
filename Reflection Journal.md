# Reflective Learning Journal – BRG27-ISEA
Student Name: Phaedra Ong Ke Qi
Student ID: CT0377433/36060240
GitHub Repository Link: https://github.com/phaedra7808-ux
Video Walkthrough Link: upload to SharePoint site: VideoUpload
FileName: CTID-YourName-AssignmentISEA.docx 
# 1. Introduction
This Reflective Learning Journal provides a comprehensive overview of the technical competencies and strategic insights developed during the four-day BRG28-ISEA Intensive Mode workshop. The primary objective of this intensive was to transition from theoretical server management to the hands-on implementation of a robust, secure, and automated Linux-based cloud infrastructure. Over the course of the session, the scope evolved from foundational local Linux installations to the deployment of enterprise-grade cloud environments utilizing industry-leading platforms such as [Insert your provider: e.g., AWS/Azure].
The technical journey documented herein covers the end-to-end lifecycle of server operations. This includes initial environment provisioning, the application of the Principle of Least Privilege (PoLP) through advanced permission management, and the implementation of Bash scripting to achieve operational efficiency. Furthermore, the journal explores critical web infrastructure components, specifically the configuration of Domain Name Systems (DNS) and the deployment of SSL/TLS certificates via Let’s Encrypt to ensure data encryption and integrity.
A significant portion of this reflection is dedicated to the economic and strategic considerations of IT infrastructure. Through a Total Cost of Ownership (TCO) analysis, this project evaluates the financial viability of various cloud providers, ensuring that technical decisions align with business scalability and cost-efficiency. Additionally, the journal reflects on a self-selected additional service—[Insert your chosen service, e.g., Docker/MariaDB]—to demonstrate the ability to independently research and deploy specialized server functions.
Ultimately, this journal serves as a synthesis of practical troubleshooting, architectural decision-making, and professional consultancy engagement. By mapping these activities to industry frameworks such as NIST and DevOps methodologies, this document demonstrates a readiness to manage modern server environments in a real-world professional capacity.

# 2. Linux Environment Setup and GitHub Integration
Through these labs, I learned how to set up a Linux Ubuntu environment from scratch. The process helped me better understand how an operating system supports software development tasks and how different system components work together as this is my first time configuring a Linux system in a structured way. By practicing the use of the terminal, I became more comfortable executing commands, and I learned how to update the system, install software packages, and navigate the Linux file system. This experience showed me how Linux offers flexibility, efficiency, and greater control for programming and development tasks. It also improved my confidence in managing system settings, installing software, and troubleshooting issues, which are important skills for real-world IT and software development environments.
GitHub Usage
I used GitHub to store and document all my lab practices throughout this module, uploading every lab I completed along with screenshots as evidence of my work. This approach helped me keep track of what I have done and when each task was completed, as GitHub automatically records all changes with clear dates and timestamps. By reviewing the commit history, I could easily monitor my progress and see how my skills developed over time. Using GitHub also allowed me to organize my work in a clear and structured manner, making it easier to revisit past labs and understand the steps I took to complete them. Overall, GitHub has been a significant help in tracking, managing, and organizing my work throughout my learning process, while also reinforcing the importance of proper documentation and version control in IT practices.

# 3. Linux Services, Permissions, and Bash Scripting
This foundational stage involved establishing a stable operating system environment by installing an Ubuntu distribution on a local machine. By integrating GitHub, the project transitioned from isolated local work to a professional version-controlled workflow. This setup ensured that all subsequent scripts and configuration changes were tracked, allowing for easy recovery and collaboration.
<img width="940" height="54" alt="image" src="https://github.com/user-attachments/assets/b43b5840-a76d-441b-bb66-b6ee5a9ed8ba" />
Scripting and Automation:
In a real-world production environment, manual configuration is considered a liability. Learning to script and schedule these functions mimics the Infrastructure as Code (IaC) philosophy, where reliability is achieved through repeatable, automated processes rather than one-off manual commands. This automation ensures system uptime and data protection even without active administrative supervision.

# 4. Cloud Infrastructure and TCO Analysis
Cloud Deployment:
My experience with cloud deployment highlighted that infrastructure management is as much about Security as it is about Connectivity. By configuring the NSG rules before the VM was even fully "Live," I followed the industry standard of "Security by Design." This prevented the common mistake of leaving a server exposed to brute-force attacks on non-essential ports. Furthermore, using a cloud provider allowed me to assign a Static Public IP or DNS label, which was a necessary prerequisite for the DNS and SSL labs that followed.
Cost Analysis:
This phase of the lab moved beyond technical configuration to address the business logic of cloud management by performing a comprehensive Total Cost of Ownership (TCO) analysis to justify a five-year infrastructure lifecycle. Using the Microsoft Azure Pricing Calculator and AWS TCO tools, I compared the financial impact of running Ubuntu instances on both platforms, specifically evaluating variables such as hourly compute rates for Free Tier eligible instances (like the Azure B1s), monthly managed disk storage costs, and potential data egress fees. By weighing these operational expenses against the flexibility and scalability features of each provider, I formulated a platform recommendation that balanced budget efficiency with technical performance. This financial evaluation was central to my Disaster Recovery (DRC) strategy, as I justified the added cost of a multi-cloud architecture—utilizing AWS as the Primary site and Azure as the Secondary site—by demonstrating that the price of redundancy is a strategic investment compared to the significant business losses incurred during a total provider outage.
<img width="940" height="543" alt="image" src="https://github.com/user-attachments/assets/d165117e-959d-420f-a37b-aa2efef1f4e4" />

# 5. DNS Setup and SSL Configuration
This stage of the lab bridged the gap between backend cloud infrastructure and the frontend user experience by linking a custom domain name to the provisioned cloud instances. I configured DNS A Records within the domain registrar's management console to point to the static public IP addresses of both the AWS (Primary) and Azure (Secondary) servers. To ensure the configuration was successful, I utilized CLI tools such as nslookup and ping to verify that the domain correctly resolved to the intended IP address. This setup was a critical prerequisite for the subsequent SSL configuration, as Let’s Encrypt requires a functional domain name to validate and issue digital certificates.

# 6. Automation and Cron Jobs
My experience with Cron jobs highlighted the immense value of automation in high-stakes IT environments. In a professional setting, relying on manual backups is a significant liability; automation transforms a "task" into a "service" that runs silently in the background. By mastering cron scheduling and script error handling, I have developed the skills necessary for DevOps and System Administration roles, where ensuring 24/7 reliability and data integrity through automated workflows is a primary responsibility.

# 7. Consulting Simulation and Additional Server Service
The experience of deploying an additional service independently served as a bridge between being a student and an IT Consultant. In industry, consultants are often tasked with integrating legacy systems with modern tools; successfully configuring Docker on a secured Ubuntu instance proved that I can bridge that gap. By considering the security implications of the service—such as managing container port mapping within my Azure Network Security Groups—I ensured that the new service did not compromise the overall integrity of the cloud infrastructure. This self-selected task built my confidence in approaching unfamiliar server projects with a structured, security-first methodology.

# 8. Problems Encountered and Solutions
The troubleshooting process reinforced the importance of systematic debugging and the use of verbose logging. In an enterprise environment, "Problem Management" is a core component of the ITIL framework; by documenting these errors and their resolutions, I have created a roadmap for future maintenance. This experience proved that most infrastructure failures stem from minor configuration oversights, such as firewall rules or pathing issues, and that a deep understanding of Linux permissions and network security layers is essential for a Cloud Architect to maintain 100% service uptime.

# 9. Industry Relevance
My work in this lab demonstrated that technical proficiency must be balanced with strategic thinking. In the industry, it is not enough to simply deploy a server; one must also ensure it is financially sustainable, globally accessible via DNS, and protected by automated recovery scripts. This holistic approach to IT management is what separates a technician from a consultant, and it is a mindset I will carry forward into my professional career.

# 10. Final Reflection
This lab series has been a transformative learning experience, shifting my technical capabilities from basic command-line navigation to the architecture of secure, automated cloud ecosystems. By successfully integrating local Linux environments with global providers like AWS and Azure, I have developed a deep understanding of how individual components—such as DNS records, SSL certificates, and Bash scripts—interlock to form a professional web infrastructure. One of the most significant takeaways was realizing that technical success is not just about making a service "work," but about ensuring it is resilient through Disaster Recovery (DRC) planning and financially sustainable through TCO Analysis. If I were to approach future projects differently, I would prioritize "Security by Design" earlier in the lifecycle, ensuring that all firewall rules and least-privilege permissions are hardened before any software is installed. Ultimately, this project has equipped me with the confidence to manage complex IT challenges with a structured, security-first methodology that balances technical performance with business objectives.

# 11. AI Tools Used (if any)
I  used Gemini to help with my sentence structure so I can present a well written reflection journal.

# 12. Appendix (Add as needed)
- GitHub repo file structure
<img width="802" height="723" alt="image" src="https://github.com/user-attachments/assets/0e1aa44e-87d0-400f-9d64-24ca799bfa98" />
